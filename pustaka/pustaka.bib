
@misc{hu_multi-scale_2023,
	title = {Multi-scale {Multi}-site {Renal} {Microvascular} {Structures} {Segmentation} for {Whole} {Slide} {Imaging} in {Renal} {Pathology}},
	url = {http://arxiv.org/abs/2308.05782},
	abstract = {Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multiscale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40×, 20×, 10×, and 5×). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.},
	language = {en},
	urldate = {2024},
	publisher = {arXiv},
	author = {Hu, Franklin and Deng, Ruining and Bao, Shunxing and Yang, Haichun and Huo, Yuankai},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05782 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Hu et al. - 2023 - Multi-scale Multi-site Renal Microvascular Structu.pdf:C\:\\Users\\junpi\\Zotero\\storage\\LG2AQYZJ\\Hu et al. - 2023 - Multi-scale Multi-site Renal Microvascular Structu.pdf:application/pdf},
}

@article{jain_advances_2023,
	title = {Advances and prospects for the {Human} {BioMolecular} {Atlas} {Program} ({HuBMAP})},
	volume = {25},
	issn = {1465-7392, 1476-4679},
	url = {https://www.nature.com/articles/s41556-023-01194-w},
	doi = {10.1038/s41556-023-01194-w},
	language = {en},
	number = {8},
	urldate = {2024},
	journal = {Nature Cell Biology},
	author = {Jain, Sanjay and Pei, Liming and Spraggins, Jeffrey M. and Angelo, Michael and Carson, James P. and Gehlenborg, Nils and Ginty, Fiona and Gonçalves, Joana P. and Hagood, James S. and Hickey, John W. and Kelleher, Neil L. and Laurent, Louise C. and Lin, Shin and Lin, Yiing and Liu, Huiping and Naba, Alexandra and Nakayasu, Ernesto S. and Qian, Wei-Jun and Radtke, Andrea and Robson, Paul and Stockwell, Brent R. and Van De Plas, Raf and Vlachos, Ioannis S. and Zhou, Mowei and {HuBMAP Consortium} and Ahn, Kyung Jin and Allen, Jamie and Anderson, David M. and Anderton, Christopher R. and Curcio, Christine and Angelin, Alessia and Arvanitis, Constadina and Atta, Lyla and Awosika-Olumo, Demi and Bahmani, Amir and Bai, Huajun and Balderrama, Karol and Balzano, Leandro and Bandyopadhyay, Gautam and Bandyopadhyay, Shovik and Bar-Joseph, Ziv and Barnhart, Kurt and Barwinska, Daria and Becich, Michael and Becker, Laren and Becker, Winston and Bedi, Kenneth and Bendall, Sean and Benninger, Kathy and Betancur, David and Bettinger, Keith and Billings, Sunteasja and Blood, Philip and Bolin, Daniel and Border, Samuel and Bosse, Marc and Bramer, Lisa and Brewer, Maya and Brusko, Maigan and Bueckle, Andreas and Burke, Karl and Burnum-Johnson, Kristin and Butcher, Eugene and Butterworth, Elizabeth and Cai, Long and Calandrelli, Riccardo and Caldwell, Michael and Campbell-Thompson, Martha and Cao, Dongfeng and Cao-Berg, Ivan and Caprioli, Richard and Caraccio, Chiara and Caron, Anita and Carroll, Megan and Chadwick, Chrystal and Chen, Angela and Chen, Derek and Chen, Fei and Chen, Haoran and Chen, Jing and Chen, Li and Chen, Lu and Chiacchia, Kenneth and Cho, Sanghee and Chou, Peter and Choy, Lisa and Cisar, Cecilia and Clair, Geremy and Clarke, Laura and Clouthier, Kelly A. and Colley, Madeline E. and Conlon, Kristin and Conroy, John and Contrepois, Kevin and Corbett, Anthony and Corwin, Alex and Cotter, Daniel and Courtois, Elise and Cruz, Aaron and Csonka, Christopher and Czupil, Kimberley and Daiya, Vicky and Dale, Kali and Davanagere, Shakeel Ahamed and Dayao, Monica and De Caestecker, Mark P. and Decker, Aubrianna and Deems, Stephen and Degnan, David and Desai, Tushar and Deshpande, Vikrant and Deutsch, Gail and Devlin, Michelle and Diep, Dinh and Dodd, Carla and Donahue, Sean and Dong, Weixiu and Dos Santos Peixoto, Rafael and Duffy, Michael and Dufresne, Martin and Duong, Thu Elizabeth and Dutra, Jennifer and Eadon, Michael T. and El-Achkar, Tarek M. and Enninful, Archibald and Eraslan, Gokcen and Eshelman, Diane and Espin-Perez, Almudena and Esplin, Edward D. and Esselman, Allison and Falo, Louis D. and Falo, Louis and Fan, Jean and Fan, Rong and Farrow, Melissa A. and Farzad, Negin and Favaro, Patricia and Fermin, Jamie and Filiz, Ferda and Filus, Shane and Fisch, Kathleen and Fisher, Eyal and Fisher, Stephen and Flowers, Katelyn and Flynn, William F. and Fogo, Agnes B. and Fu, Dongtao and Fulcher, James and Fung, Anthony and Furst, Derek and Gallant, Michael and Gao, Fu and Gao, Yu and Gaulton, Kyle and Gaut, Joseph P. and Gee, James and Ghag, Reetika R. and Ghazanfar, Shila and Ghose, Soumya and Gisch, Debora and Gold, Ilan and Gondalia, Aashay and Gorman, Brittney and Greenleaf, William and Greenwald, Noah and Gregory, Brian and Guo, Rong and Gupta, Rajat and Hakimian, Hunter and Haltom, Jeff and Halushka, Marc and Han, Kyu Sang and Hanson, Casey and Harbury, Pehr and Hardi, Josef and Harlan, Linda and Harris, Raymond C. and Hartman, Austin and Heidari, Elyas and Helfer, Jesse and Helminiak, David and Hemberg, Martin and Henning, Nathaniel and Herr, Bruce W. and Ho, Jonhan and Holden-Wiltse, Jeanne and Hong, Seung-Hyun and Hong, Young-Kwon and Honick, Brendan and Hood, Greg and Hu, Po and Hu, Qiwen and Huang, Molly and Huyck, Heidie and Imtiaz, Tamjid and Isberg, Olof Gerdur and Itkin, Maxim and Jackson, Dana and Jacobs, Marni and Jain, Yashvardhan and Jewell, David and Jiang, Lihua and Jiang, Zhenghui G. and Johnston, Sarah and Joshi, Pujan and Ju, Yingnan and Judd, Audra and Kagel, Adam and Kahn, Ari and Kalavros, Nikolaos and Kalhor, Kian and Karagkouni, Dimitra and Karathanos, Thomas and Karunamurthy, Arivarasan and Katari, Suhas and Kates, Heather and Kaushal, Madhurima and Keener, Nicholas and Keller, Mark and Kenney, Mariah and Kern, Colin and Kharchenko, Peter and Kim, Junhyong and Kingsford, Carl and Kirwan, Jessica and Kiselev, Vladimir and Kishi, Jocelyn and Kitata, Reta Birhanu and Knoten, Amanda and Kollar, Charles and Krishnamoorthy, Praveen and Kruse, Angela R. S. and Da, Kuang and Kundaje, Anshul and Kutschera, Eric and Kwon, Yumi and Lake, Blue B. and Lancaster, Samuel and Langlieb, Jonah and Lardenoije, Roy and Laronda, Monica and Laskin, Julia and Lau, Ken and Lee, Hayan and Lee, Maria and Lee, Mejeong and Strekalova, Yulia Levites and Li, Dongshunyi and Li, Jennifer and Li, Jilong and Li, Xiangtang and Li, Zhi and Liao, Yen-Chen and Liaw, Tiffany and Lin, Pei and Lin, Yulieh and Lindsay, Scott and Liu, Chunjie and Liu, Yang and Liu, Yuan and Lott, Marie and Lotz, Martin and Lowery, Lisa and Lu, Peiran and Lu, Xinyue and Lucarelli, Nicholas and Lun, Xiaokang and Luo, Zhifei and Ma, Jian and Macosko, Evan and Mahajan, Mayank and Maier, Libby and Makowski, Danika and Malek, Morad and Manthey, David and Manz, Trevor and Margulies, Kenneth and Marioni, John and Martindale, Matthew and Mason, Cayla and Mathews, Clayton and Maye, Peter and McCallum, Chuck and McDonough, Elizabeth and McDonough, Liz and Mcdowell, Hannah and Meads, Morgan and Medina-Serpas, Miguel and Ferreira, Ricardo Melo and Messinger, Jeffrey and Metis, Kay and Migas, Lukasz G. and Miller, Brendan and Mimar, Sayat and Minor, Brittany and Misra, Ravi and Missarova, Alsu and Mistretta, Christopher and Moens, Roger and Moerth, Eric and Moffitt, Jeffrey and Molla, Gesmira and Monroe, Matthew and Monte, Emma and Morgan, Mike and Muraro, Daniele and Murphy, Bob and Murray, Evan and Musen, Mark A. and Naglah, Ahmed and Nasamran, Chanond and Neelakantan, Taruna and Nevins, Stephanie and Nguyen, Hieu and Nguyen, Nam and Nguyen, Tram and Nguyen, Tri and Nigra, Deb and Nofal, Michel and Nolan, Garry and Nwanne, Gerald and O’Connor, Martin and Okuda, Kenichi and Olmer, Merissa and O’Neill, Kathleen and Otaluka, Nancy and Pang, Minxing and Parast, Mana and Pasa-Tolic, Ljiljana and Paten, Benedict and Patterson, Nathan Heath and Peng, Ting and Phillips, Gesina and Pichavant, Mina and Piehowski, Paul and Pilner, Hannah and Pingry, Ellie and Pita-Juarez, Yered and Plevritis, Sylvia and Ploumakis, Athanasios and Pouch, Alison and Pryhuber, Gloria and Puerto, Juan and Qaurooni, Danial and Qin, Ling and Quardokus, Ellen M. and Rajbhandari, Presha and Rakow-Penner, Rebecca and Ramasamy, Ramalakshmi and Read, David and Record, Elizabeth G. and Reeves, David and Ricarte, Allyson and Rodríguez-Soto, Ana and Ropelewski, Alexander and Rosario, Jean and Roselkis, Morla-Adames and Rowe, David and Roy, Tarun Kanti and Ruffalo, Matt and Ruschman, Nancy and Sabo, Angela and Sachdev, Nina and Saka, Sinem and Salamon, Diane and Sarder, Pinaki and Sasaki, Hiroshi and Satija, Rahul and Saunders, Diane and Sawka, Riley and Schey, Kevin and Schlehlein, Heidi and Scholten, David and Schultz, Sarah and Schwartz, Lauren and Schwenk, Melissa and Scibek, Robin and Segre, Ayellet and Serrata, Matthew and Shands, Walter and Shen, Xiaotao and Shendure, Jay and Shephard, Holly and Shi, Lingyan and Shi, Tujin and Shin, Dong-Guk and Shirey, Bill and Sibilla, Max and Silber, Michal and Silverstein, Jonathan and Simmel, Derek and Simmons, Alan and Singhal, Dhruv and Sivajothi, Santhosh and Smits, Thomas and Soncin, Francesca and Song, Qi and Stanley, Valentina and Stuart, Tim and Su, Hanquan and Su, Pei and Sun, Xin and Surrette, Christine and Swahn, Hannah and Tan, Kai and Teichmann, Sarah and Tejomay, Abhiroop and Tellides, George and Thomas, Kathleen and Thomas, Tracey and Thompson, Marissa and Tian, Hua and Tideman, Leonoor and Trapnell, Cole and Tsai, Albert G. and Tsai, Chia-Feng and Tsai, Leo and Tsui, Elizabeth and Tsui, Tina and Tung, Jason and Turner, Morgan and Uranic, Jackie and Vaishnav, Eeshit Dhaval and Varra, Sricharan Reddy and Vaskivskyi, Vasyl and Velickovic, Dusan and Velickovic, Marija and Verheyden, Jamie and Waldrip, Jessica and Wallace, Douglas and Wan, Xueyi and Wang, Allen and Wang, Fusheng and Wang, Meng and Wang, Shuoshuo and Wang, Xuefei and Wasserfall, Clive and Wayne, Leonard and Webber, James and Weber, Griffin M. and Wei, Bei and Wei, Jian-Jun and Weimer, Annika and Welling, Joel and Wen, Xingzhao and Wen, Zishen and Williams, MacKenzie and Winfree, Seth and Winograd, Nicholas and Woodard, Abashai and Wright, Devin and Wu, Fan and Wu, Pei-Hsun and Wu, Qiuyang and Wu, Xiaodong and Xing, Yi and Xu, Tianyang and Yang, Manxi and Yang, Mingyu and Yap, Joseph and Ye, Dong Hye and Yin, Peng and Yuan, Zhou and Yun, Chi and Zahraei, Ali and Zemaitis, Kevin and Zhang, Bo and Zhang, Caibin and Zhang, Chenyu and Zhang, Chi and Zhang, Kun and Zhang, Shiping and Zhang, Ted and Zhang, Yida and Zhao, Bingqing and Zhao, Wenxin and Zheng, Jia Wen and Zhong, Sheng and Zhu, Bokai and Zhu, Chenchen and Zhu, Diming and Zhu, Quan and Zhu, Ying and Börner, Katy and Snyder, Michael P.},
	month = aug,
	year = {2023},
	pages = {1089--1100},
	file = {Jain et al. - 2023 - Advances and prospects for the Human BioMolecular .pdf:C\:\\Users\\junpi\\Zotero\\storage\\8X3MUM8C\\Jain et al. - 2023 - Advances and prospects for the Human BioMolecular .pdf:application/pdf},
}

@article{lake_atlas_2023,
	title = {An atlas of healthy and injured cell states and niches in the human kidney},
	volume = {619},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-023-05769-3},
	doi = {10.1038/s41586-023-05769-3},
	abstract = {Abstract
            
              Understanding kidney disease relies on defining the complexity of cell types and states, their associated molecular profiles and interactions within tissue neighbourhoods
              1
              . Here we applied multiple single-cell and single-nucleus assays ({\textgreater}400,000 nuclei or cells) and spatial imaging technologies to a broad spectrum of healthy reference kidneys (45 donors) and diseased kidneys (48 patients). This has provided a high-resolution cellular atlas of 51 main cell types, which include rare and previously undescribed cell populations. The multi-omic approach provides detailed transcriptomic profiles, regulatory factors and spatial localizations spanning the entire kidney. We also define 28 cellular states across nephron segments and interstitium that were altered in kidney injury, encompassing cycling, adaptive (successful or maladaptive repair), transitioning and degenerative states. Molecular signatures permitted the localization of these states within injury neighbourhoods using spatial transcriptomics, while large-scale 3D imaging analysis (around 1.2 million neighbourhoods) provided corresponding linkages to active immune responses. These analyses defined biological pathways that are relevant to injury time-course and niches, including signatures underlying epithelial repair that predicted maladaptive states associated with a decline in kidney function. This integrated multimodal spatial cell atlas of healthy and diseased human kidneys represents a comprehensive benchmark of cellular states, neighbourhoods, outcome-associated signatures and publicly available interactive visualizations.},
	language = {en},
	number = {7970},
	urldate = {2024},
	journal = {Nature},
	author = {Lake, Blue B. and Menon, Rajasree and Winfree, Seth and Hu, Qiwen and Melo Ferreira, Ricardo and Kalhor, Kian and Barwinska, Daria and Otto, Edgar A. and Ferkowicz, Michael and Diep, Dinh and Plongthongkum, Nongluk and Knoten, Amanda and Urata, Sarah and Mariani, Laura H. and Naik, Abhijit S. and Eddy, Sean and Zhang, Bo and Wu, Yan and Salamon, Diane and Williams, James C. and Wang, Xin and Balderrama, Karol S. and Hoover, Paul J. and Murray, Evan and Marshall, Jamie L. and Noel, Teia and Vijayan, Anitha and Hartman, Austin and Chen, Fei and Waikar, Sushrut S. and Rosas, Sylvia E. and Wilson, Francis P. and Palevsky, Paul M. and Kiryluk, Krzysztof and Sedor, John R. and Toto, Robert D. and Parikh, Chirag R. and Kim, Eric H. and Satija, Rahul and Greka, Anna and Macosko, Evan Z. and Kharchenko, Peter V. and Gaut, Joseph P. and Hodgin, Jeffrey B. and {KPMP Consortium} and Knight, Richard and Lecker, Stewart H. and Stillman, Isaac and Amodu, Afolarin A. and Ilori, Titlayo and Maikhor, Shana and Schmidt, Insa and McMahon, Gearoid M. and Weins, Astrid and Hacohen, Nir and Bush, Lakeshia and Gonzalez-Vicente, Agustin and Taliercio, Jonathan and O’toole, John and Poggio, Emilio and Cooperman, Leslie and Jolly, Stacey and Herlitz, Leal and Nguyen, Jane and Palmer, Ellen and Sendrey, Dianna and Spates-Harden, Kassandra and Appelbaum, Paul and Barasch, Jonathan M. and Bomback, Andrew S. and D’Agati, Vivette D. and Mehl, Karla and Canetta, Pietro A. and Shang, Ning and Balderes, Olivia and Kudose, Satoru and Barisoni, Laura and Alexandrov, Theodore and Cheng, Yinghua and Dunn, Kenneth W. and Kelly, Katherine J. and Sutton, Timothy A. and Wen, Yumeng and Corona-Villalobos, Celia P. and Menez, Steven and Rosenberg, Avi and Atta, Mohammed and Johansen, Camille and Sun, Jennifer and Roy, Neil and Williams, Mark and Azeloglu, Evren U. and He, Cijang and Iyengar, Ravi and Hansen, Jens and Xiong, Yuguang and Rovin, Brad and Parikh, Samir and Madhavan, Sethu M. and Anderton, Christopher R. and Pasa-Tolic, Ljiljana and Velickovic, Dusan and Troyanskaya, Olga and Sealfon, Rachel and Tuttle, Katherine R. and Laszik, Zoltan G. and Nolan, Garry and Sarwal, Minnie and Anjani, Kavya and Sigdel, Tara and Ascani, Heather and Balis, Ulysses G. J. and Lienczewski, Chrysta and Steck, Becky and He, Yougqun and Schaub, Jennifer and Blanc, Victoria M. and Murugan, Raghavan and Randhawa, Parmjeet and Rosengart, Matthew and Tublin, Mitchell and Vita, Tina and Kellum, John A. and Hall, Daniel E. and Elder, Michele M. and Winters, James and Gilliam, Matthew and Alpers, Charles E. and Blank, Kristina N. and Carson, Jonas and De Boer, Ian H. and Dighe, Ashveena L. and Himmelfarb, Jonathan and Mooney, Sean D. and Shankland, Stuart and Williams, Kayleen and Park, Christopher and Dowd, Frederick and McClelland, Robyn L. and Daniel, Stephen and Hoofnagle, Andrew N. and Wilcox, Adam and Bansal, Shweta and Sharma, Kumar and Venkatachalam, Manjeri and Zhang, Guanshi and Pamreddy, Annapurna and Kakade, Vijaykumar R. and Moledina, Dennis and Shaw, Melissa M. and Ugwuowo, Ugochukwu and Arora, Tanima and Ardayfio, Joseph and Bebiak, Jack and Brown, Keith and Campbell, Catherine E. and Saul, John and Shpigel, Anna and Stutzke, Christy and Koewler, Robert and Campbell, Taneisha and Hayashi, Lynda and Jefferson, Nichole and Pinkeney, Roy and Roberts, Glenda V. and Eadon, Michael T. and Dagher, Pierre C. and El-Achkar, Tarek M. and Zhang, Kun and Kretzler, Matthias and Jain, Sanjay},
	month = jul,
	year = {2023},
	pages = {585--594},
	file = {Lake et al. - 2023 - An atlas of healthy and injured cell states and ni.pdf:C\:\\Users\\junpi\\Zotero\\storage\\6QB95W35\\Lake et al. - 2023 - An atlas of healthy and injured cell states and ni.pdf:application/pdf},
}

@article{weber_considerations_2020,
	title = {Considerations for {Using} the {Vasculature} as a {Coordinate} {System} to {Map} {All} the {Cells} in the {Human} {Body}},
	volume = {7},
	issn = {2297-055X},
	url = {https://www.frontiersin.org/article/10.3389/fcvm.2020.00029/full},
	doi = {10.3389/fcvm.2020.00029},
	abstract = {Several ongoing international efforts are developing methods of localizing single cells within organs or mapping the entire human body at the single cell level, including the Chan Zuckerberg Initiative’s Human Cell Atlas (HCA), and the Knut and Allice Wallenberg Foundation’s Human Protein Atlas (HPA), and the National Institutes of Health’s Human BioMolecular Atlas Program (HuBMAP). Their goals are to understand cell specialization, interactions, spatial organization in their natural context, and ultimately the function of every cell within the body. In the same way that the Human Genome Project had to assemble sequence data from different people to construct a complete sequence, multiple centers around the world are collecting tissue specimens from diverse populations that vary in age, race, sex, and body size. A challenge will be combining these heterogeneous tissue samples into a 3D reference map that will enable multiscale, multidimensional Google Maps-like exploration of the human body. Key to making alignment of tissue samples work is identifying and using a coordinate system called a Common Coordinate Framework (CCF), which deﬁnes the positions, or “addresses,” in a reference body, from whole organs down to functional tissue units and individual cells. In this perspective, we examine the concept of a CCF based on the vasculature and describe why it would be an attractive choice for mapping the human body.},
	language = {en},
	urldate = {2024},
	journal = {Frontiers in Cardiovascular Medicine},
	author = {Weber, Griffin M. and Ju, Yingnan and Börner, Katy},
	month = mar,
	year = {2020},
	pages = {29},
	file = {Weber et al. - 2020 - Considerations for Using the Vasculature as a Coor.pdf:C\:\\Users\\junpi\\Zotero\\storage\\EML594K7\\Weber et al. - 2020 - Considerations for Using the Vasculature as a Coor.pdf:application/pdf},
}

@incollection{galis_vasculome_2022,
	title = {The {Vasculome} provides a body-wide cellular positioning system and functional barometer. {The} “{Vasculature} as {Common} {Coordinate} {Frame} ({VCCF})” concept},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	isbn = {978-0-12-822546-2},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B978012822546200037X},
	language = {en},
	urldate = {2024},
	booktitle = {The {Vasculome}},
	publisher = {Elsevier},
	author = {Galis, Zorina S.},
	year = {2022},
	doi = {10.1016/B978-0-12-822546-2.00037-X},
	pages = {453--460},
}

@article{huang_fully_2022,
	title = {Fully {Convolutional} {Network} for the {Semantic} {Segmentation} of {Medical} {Images}: {A} {Survey}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2075-4418},
	shorttitle = {Fully {Convolutional} {Network} for the {Semantic} {Segmentation} of {Medical} {Images}},
	url = {https://www.mdpi.com/2075-4418/12/11/2765},
	doi = {10.3390/diagnostics12112765},
	abstract = {There have been major developments in deep learning in computer vision since the 2010s. Deep learning has contributed to a wealth of data in medical image processing, and semantic segmentation is a salient technique in this ﬁeld. This study retrospectively reviews recent studies on the application of deep learning for segmentation tasks in medical imaging and proposes potential directions for future development, including model development, data augmentation processing, and dataset creation. The strengths and deﬁciencies of studies on models and data augmentation, as well as their application to medical image segmentation, were analyzed. Fully convolutional network developments have led to the creation of the U-Net and its derivatives. Another noteworthy image segmentation model is DeepLab. Regarding data augmentation, due to the low data volume of medical images, most studies focus on means to increase the wealth of medical image data. Generative adversarial networks (GAN) increase data volume via deep learning. Despite the increasing types of medical image datasets, there is still a deﬁciency of datasets on speciﬁc problems, which should be improved moving forward. Considering the wealth of ongoing research on the application of deep learning processing to medical image segmentation, the data volume and practical clinical application problems must be addressed to ensure that the results are properly applied.},
	language = {en},
	number = {11},
	urldate = {2024},
	journal = {Diagnostics},
	author = {Huang, Sheng-Yao and Hsu, Wen-Lin and Hsu, Ren-Jun and Liu, Dai-Wei},
	month = nov,
	year = {2022},
	pages = {2765},
	file = {Huang et al. - 2022 - Fully Convolutional Network for the Semantic Segme.pdf:C\:\\Users\\junpi\\Zotero\\storage\\8YI2WFFC\\Huang et al. - 2022 - Fully Convolutional Network for the Semantic Segme.pdf:application/pdf},
}

@inproceedings{cootes_statistical_2001,
	address = {San Diego, CA},
	title = {Statistical models of appearance for medical image analysis and computer vision},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=906296},
	doi = {10.1117/12.431093},
	urldate = {2024},
	author = {Cootes, Tim F. and Taylor, Christopher J.},
	editor = {Sonka, Milan and Hanson, Kenneth M.},
	month = jul,
	year = {2001},
	pages = {236--248},
	file = {236.pdf:C\:\\Users\\junpi\\Zotero\\storage\\JS5JE3W8\\236.pdf:application/pdf},
}

@misc{cai_ma-unet_2020,
	title = {{MA}-{Unet}: {An} improved version of {Unet} based on multi-scale and attention mechanism for medical image segmentation},
	shorttitle = {{MA}-{Unet}},
	url = {http://arxiv.org/abs/2012.10952},
	abstract = {Although convolutional neural networks (CNNs) are promoting the development of medical image semantic segmentation, the standard model still has some shortcomings. First, the feature mapping from the encoder and decoder sub-networks in the skip connection operation has a large semantic diﬀerence. Second, the remote feature dependence is not eﬀectively modeled. Third, the global context information of diﬀerent scales is ignored. In this paper, we try to eliminate semantic ambiguity in skip connection operations by adding attention gates (AGs), and use attention mechanisms to combine local features with their corresponding global dependencies, explicitly model the dependencies between channels and use multiscale predictive fusion to utilize global information at diﬀerent scales. Compared with other state-of-theart segmentation networks, our model obtains better segmentation performance while introducing fewer parameters.},
	language = {en},
	urldate = {2024},
	publisher = {arXiv},
	author = {Cai, Yutong and Wang, Yong},
	month = dec,
	year = {2020},
	note = {arXiv:2012.10952 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Cai and Wang - 2020 - MA-Unet An improved version of Unet based on mult.pdf:C\:\\Users\\junpi\\Zotero\\storage\\HF34SGMS\\Cai and Wang - 2020 - MA-Unet An improved version of Unet based on mult.pdf:application/pdf},
}

@article{yousefi_esophageal_2021,
	title = {Esophageal {Tumor} {Segmentation} in {CT} {Images} {Using} a {Dilated} {Dense} {Attention} {Unet} ({DDAUnet})},
	volume = {9},
	abstract = {Manual or automatic delineation of the esophageal tumor in CT images is known to be very challenging. This is due to the low contrast between the tumor and adjacent tissues, the anatomical variation of the esophagus, as well as the occasional presence of foreign bodies (e.g. feeding tubes). Physicians therefore usually exploit additional knowledge such as endoscopic ﬁndings, clinical history, additional imaging modalities like PET scans. Achieving his additional information is time-consuming, while the results are error-prone and might lead to non-deterministic results. In this paper we aim to investigate if and to what extent a simpliﬁed clinical workﬂow based on CT alone, allows one to automatically segment the esophageal tumor with sufﬁcient quality. For this purpose, we present a fully automatic end-to-end esophageal tumor segmentation method based on convolutional neural networks (CNNs). The proposed network, called Dilated Dense Attention Unet (DDAUnet), leverages spatial and channel attention gates in each dense block to selectively concentrate on determinant feature maps and regions. Dilated convolutional layers are used to manage GPU memory and increase the network receptive ﬁeld. We collected a dataset of 792 scans from 288 distinct patients including varying anatomies with air pockets, feeding tubes and proximal tumors. Repeatability and reproducibility studies were conducted for three distinct splits of training and validation sets. The proposed network achieved a DSC value of 0.79 ± 0.20, a mean surface distance of 5.4 ± 20.2mm and 95\% Hausdorff distance of 14.7 ± 25.0mm for 287 test scans, demonstrating promising results with a simpliﬁed clinical workﬂow based on CT alone. Our code is publicly available via https://github.com/yousefis/DenseUnet\_Esophagus\_Segmentation.},
	language = {en},
	author = {Yousefi, Sahar and Sokooti, Hessam and Elmahdy, Mohamed S and Lips, Irene M and Shalmani, Mohammad T Manzuri and Zinkstok, Roel T},
	year = {2021},
	file = {Yousefi et al. - 2021 - Esophageal Tumor Segmentation in CT Images Using a.pdf:C\:\\Users\\junpi\\Zotero\\storage\\MVTQ7XHH\\Yousefi et al. - 2021 - Esophageal Tumor Segmentation in CT Images Using a.pdf:application/pdf},
}

@misc{noor_survey_2024,
	title = {A {Survey} on {Deep} {Learning} and {State}-of-the-art {Applications}},
	url = {http://arxiv.org/abs/2403.17561},
	abstract = {Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm’s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing. We highlight the key features of the models and their effectiveness in solving the problems within each domain. Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures. Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.},
	language = {en},
	urldate = {2024},
	publisher = {arXiv},
	author = {Noor, Mohd Halim Mohd and Ige, Ayokunle Olalekan},
	month = mar,
	year = {2024},
	note = {arXiv:2403.17561 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Noor and Ige - 2024 - A Survey on Deep Learning and State-of-the-art App.pdf:C\:\\Users\\junpi\\Zotero\\storage\\8ZITPZJB\\Noor and Ige - 2024 - A Survey on Deep Learning and State-of-the-art App.pdf:application/pdf},
}

@misc{deng_omni-seg_2022,
	title = {Omni-{Seg}: {A} {Single} {Dynamic} {Network} for {Multi}-label {Renal} {Pathology} {Image} {Segmentation} using {Partially} {Labeled} {Data}},
	shorttitle = {Omni-{Seg}},
	url = {http://arxiv.org/abs/2112.12665},
	abstract = {Computer-assisted quantitative analysis on Giga-pixel pathology images has provided a new avenue in histology examination. The innovations have been largely focused on cancer pathology (i.e., tumor segmentation and characterization). In non-cancer pathology, the learning algorithms can be asked to examine more comprehensive tissue types simultaneously, as a multi-label setting. The prior arts typically needed to train multiple segmentation networks in order to match the domain-speciﬁc knowledge for heterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal tubular, distal tubular, peritubular capillaries, and arteries). In this paper, we propose a dynamic single segmentation network (Omni-Seg) that learns to segment multiple tissue types using partially labeled images (i.e., only one tissue type is labeled for each training image) for renal pathology. By learning from 150,000 patch-wise pathological images from six tissue types, the proposed Omni-Seg network achieved superior segmentation accuracy and less resource consumption when compared to the previous the multiple-network and multi-head design. In the testing stage, the proposed method obtains “completely labeled” tissue segmentation results using only “partially labeled” training images. The source code is available at https://github.com/ddrrnn123/Omni-Seg.},
	language = {en},
	urldate = {2024},
	publisher = {arXiv},
	author = {Deng, Ruining and Liu, Quan and Cui, Can and Asad, Zuhayr and Yang, Haichun and Huo, Yuankai},
	month = mar,
	year = {2022},
	note = {arXiv:2112.12665 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Deng et al. - 2022 - Omni-Seg A Single Dynamic Network for Multi-label.pdf:C\:\\Users\\junpi\\Zotero\\storage\\UG9ZDKTA\\Deng et al. - 2022 - Omni-Seg A Single Dynamic Network for Multi-label.pdf:application/pdf},
}
